{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "from wikiwho_wrapper import WikiWho\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from scripts.wiki import Wiki,Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki:\n",
    "    '''\n",
    "    MAIN CLASS TO store all revisions for a wiki along with editors and timestamp.\n",
    "    '''\n",
    "    def __init__(self,title, revs, all_tokens=[]):\n",
    "        #self.id = id\n",
    "        self.title = title\n",
    "        self.revisions = revs\n",
    "        self.add_all_token(all_tokens)\n",
    "        \n",
    "\n",
    "           \n",
    "    def add_all_token(self, all_tokens):\n",
    "        \n",
    "        for token in all_tokens:\n",
    "            self.revisions.loc[token[\"o_rev_id\"]].added.add(token[\"token_id\"])\n",
    "            for in_revision in token[\"in\"]:\n",
    "                self.revisions.loc[in_revision].added.add(token[\"token_id\"])\n",
    "            for out_revision in token[\"out\"]:\n",
    "                self.revisions.loc[out_revision].removed.add(token[\"token_id\"])\n",
    "                \n",
    "    def create_change(self, from_rev_id, to_rev_id, to_rev_content, epsilon_size):\n",
    "        try:\n",
    "            from_rev = self.revisions[from_rev_id]\n",
    "            to_rev = self.revisions[to_rev_id]\n",
    "            from_rev.deleted(to_rev)\n",
    "            to_rev.content = to_rev_content\n",
    "            to_rev.inserted_continuous_pos()\n",
    "            to_rev.inserted_neighbours()\n",
    "            from_rev.create_change_object(to_rev)\n",
    "            from_rev.append_neighbour_vec(to_rev, epsilon_size)\n",
    "        except:\n",
    "            print(\"exception occurred in calculating change object\",traceback.format_exc())\n",
    "            print(\"problem in \", to_rev_content.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Revision:\n",
    "    def __init__(self, id, timestamp,editor):\n",
    "        self.id = id\n",
    "        self.timestamp = timestamp\n",
    "        self.editor = editor\n",
    "        self.added = set()\n",
    "        self.removed = set()   \n",
    "        \n",
    "    def deleted(self, to_rev):\n",
    "        self.content[\"removed\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(to_rev.removed), assume_unique= True ))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        start_neighbour = start_pos - 1\n",
    "        end_neighbour = end_pos + 1\n",
    "        self.deleted_object = pd.DataFrame(np.c_[ start_pos, end_pos, start_neighbour, end_neighbour ],\n",
    "                                       columns=[ \"del_start_pos\", \"del_end_pos\", \"left_neigh\", \"right_neigh\",])\n",
    "    \n",
    "    def inserted_continuous_pos(self):\n",
    "        self.content[\"added\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(self.added), assume_unique= True))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        self.added_pos = np.c_[start_pos, end_pos]\n",
    "\n",
    "    def inserted_neighbours(self):\n",
    "        start_token_pos = self.added_pos[:,0] - 1\n",
    "        end_token_pos = self.added_pos[:,1] + 1\n",
    "        self.start_token_id = self.content[\"token_id\"].values[start_token_pos]\n",
    "        self.end_token_id = self.content[\"token_id\"].values[end_token_pos]\n",
    "    \n",
    "    def create_change_object(self, to_rev):\n",
    "        self.ins_left = np.argwhere(np.isin(self.content.token_id.values, to_rev.start_token_id, assume_unique= True))\n",
    "        self.ins_right = np.argwhere(np.isin(self.content.token_id.values, to_rev.end_token_id, assume_unique= True))\n",
    "        self.inserted_object = pd.DataFrame(np.concatenate([to_rev.added_pos, self.ins_left, self.ins_right], axis=1),\n",
    "                                       columns=[\"ins_start_pos\", \"ins_end_pos\", \"left_neigh\", \"right_neigh\" ])\n",
    "\n",
    "        self.change = pd.merge(self.inserted_object, self.deleted_object,how=\"outer\", on=[\"left_neigh\", \"right_neigh\"])\n",
    "        self.change.fillna(-1, inplace=True)\n",
    "        \n",
    "    def append_neighbour_vec(self, to_rev, epsilon_size):\n",
    "        self.wiki_who_tokens = self.content.token_id.values\n",
    "        del self.content\n",
    "        neighbour_df = self.change.apply(find_tokens, axis=1, args=(self, to_rev, epsilon_size))\n",
    "        neighbour_df.columns= [\"ins_tokens\", \"del_tokens\", \"left_neigh_slice\", \"right_neigh_slice\", \"left_token\", \"right_token\"]\n",
    "        self.change_df = pd.concat([self.change, neighbour_df], sort=False, axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens(change, revision, to_rev, epsilon_size):\n",
    "    start_left = (int(change[\"left_neigh\"]) - epsilon_size)\n",
    "    if start_left <0:\n",
    "        start_left = 0\n",
    "    left_neigh = slice( start_left, int(change[\"left_neigh\"]) + 1)\n",
    "    \n",
    "    end_right = (int(change[\"right_neigh\"]) + epsilon_size+1)\n",
    "    if end_right >= revision.wiki_who_tokens.size:\n",
    "        end_right = revision.wiki_who_tokens.size - 1\n",
    "    right_neigh = slice(int(change[\"right_neigh\"]), end_right )\n",
    "    if(change[\"ins_start_pos\"]==-1):\n",
    "        ins_tokens = []\n",
    "    else:\n",
    "        ins_slice = slice(int(change[\"ins_start_pos\"]), int(change[\"ins_end_pos\"]+1) )\n",
    "        ins_tokens = to_rev.content.token_id.values[ins_slice]\n",
    "    if(change[\"del_start_pos\"] == -1):\n",
    "        del_tokens = []\n",
    "    else:\n",
    "        del_slice = slice(int(change[\"del_start_pos\"]), int(change[\"del_end_pos\"]+1) )\n",
    "        del_tokens = revision.wiki_who_tokens[del_slice]\n",
    "    left_token = revision.wiki_who_tokens[left_neigh]\n",
    "    right_token = revision.wiki_who_tokens[right_neigh]\n",
    "    return pd.Series([tuple(ins_tokens), tuple(del_tokens), left_neigh, right_neigh, tuple(left_token), tuple(right_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://api.wikiwho.net/en/api/v1.0.0-beta/\"\n",
    "article_name = \"Bioglass\"\n",
    "filename = article_name + \".h5\"\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "\n",
    "epsilon_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    #retrieving all rev list and change object from file\n",
    "    rev_list = store.get(\"rev_list\")[\"id\"].values.tolist()\n",
    "    keys = [\"r\" +  str(rev) for rev in rev_list]\n",
    "    rev_len_list = [store.get(key).shape[0] for key in keys]\n",
    "rev_len_df = pd.DataFrame({\"rev_id\":rev_list[:-1], \"length\": rev_len_list[:-1]})\n",
    "\n",
    "rev_len_df.to_hdf(len_file_path, \"rev_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# with pd.HDFStore(filepath, 'r') as store:\n",
    "#     #retrieving all rev list and change object from file\n",
    "#     rev_list = store.get(\"rev_list\")\n",
    "#     all_rev = store.get(\"all_tokens\")\n",
    "#     all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "#     #making revision objects\n",
    "#     revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "#     revs.index = rev_list.id\n",
    "#     from_rev_id = revs.index[0]\n",
    "    \n",
    "#     wiki = Wiki(2345, content, revs, all_tokens)\n",
    "#     wiki.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "#     for to_rev_id in list(revs.index[1:]):\n",
    "#         key=\"r\"+str(to_rev_id)\n",
    "#         to_rev_content = store[key]\n",
    "#         wiki\n",
    "#         wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "#         from_rev_id = to_rev_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_filepath = os.path.join(change_object_dir, content+\".pkl\")\n",
    "# with open(save_filepath, \"wb\") as file:\n",
    "#     pickle.dump(wiki, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving change object for all the articles in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_change_object(article_name, content_dir = \"../data/content/\", \n",
    "                            change_object_dir =  \"../data/change objects/\", epsilon_size=30, save=False):\n",
    "    \n",
    "    content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "    change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "    \n",
    "    with pd.HDFStore(content_filepath, 'r') as store:\n",
    "        #retrieving all rev list and change object from file\n",
    "        rev_list = store.get(\"rev_list\")\n",
    "        all_rev = store.get(\"all_tokens\")\n",
    "        all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "        \n",
    "        #making revision objects\n",
    "        revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "        revs.index = rev_list.id\n",
    "        \n",
    "        # Getting first revision object and adding content ot it\n",
    "        from_rev_id = revs.index[0]\n",
    "        wiki = Wiki(article_name, revs, all_tokens)\n",
    "\n",
    "        wiki.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "        # adding content to all other revision and finding change object between them.\n",
    "        \n",
    "        for to_rev_id in list(revs.index[1:]):\n",
    "            key=\"r\"+str(to_rev_id)\n",
    "            to_rev_content = store[key]\n",
    "            wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "            from_rev_id = to_rev_id\n",
    "         \n",
    "    if save:\n",
    "        with open(change_object_filepath, \"wb\") as file:\n",
    "            pickle.dump(wiki, file)\n",
    "        \n",
    "    return wiki\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = 'Bioglass'\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir = \"../data/change objects/\"\n",
    "epsilon_size=30\n",
    "save=False\n",
    "\n",
    "content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "\n",
    "with pd.HDFStore(content_filepath, 'r') as store:\n",
    "    #retrieving all rev list and change object from file\n",
    "    rev_list = store.get(\"rev_list\")\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "    all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "\n",
    "    #making revision objects\n",
    "    revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "    revs.index = rev_list.id\n",
    "\n",
    "    # Getting first revision object and adding content ot it\n",
    "    from_rev_id = revs.index[0]\n",
    "    wiki2 = Wiki(article_name, revs, all_tokens)\n",
    "\n",
    "    wiki2.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "    # adding content to all other revision and finding change object between them.\n",
    "\n",
    "    for to_rev_id in list(revs.index[1:]):\n",
    "        key=\"r\"+str(to_rev_id)\n",
    "        to_rev_content = store[key]\n",
    "        wiki2.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "        from_rev_id = to_rev_id\n",
    "\n",
    "if save:\n",
    "    with open(change_object_filepath, \"wb\") as file:\n",
    "        pickle.dump(wiki2, file)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract this data using the wikiwho wrapper instead of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def was_added(x, revision):\n",
    "    i = x[\"in\"]\n",
    "    o = x[\"out\"]\n",
    "    i_cleaned = []\n",
    "    o_cleaned = []\n",
    "    for rev in i:\n",
    "        if rev <= revision:\n",
    "            i_cleaned.append(rev)\n",
    "    for rev in o:\n",
    "        if rev <= revision:\n",
    "            o_cleaned.append(rev)\n",
    "    if (len(i_cleaned) == len(o_cleaned)):\n",
    "        x.added = True\n",
    "    return x\n",
    "\n",
    "def add_start_end_token(df, add_false=False):\n",
    "    if add_false:\n",
    "        df.loc[len(df)] = [\"{$nd}\", -2, False]\n",
    "        df.loc[-1] = [\"{st@rt}\", -1, False]\n",
    "    else:\n",
    "        df.loc[len(df)] = [\"{$nd}\", -2]\n",
    "        df.loc[-1] = [\"{st@rt}\", -1]      \n",
    "    df.index = df.index+1\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def df_rev_content(key, first=False):\n",
    "    rev_content = ww.api.specific_rev_content_by_rev_id(key, o_rev_id=False, editor=False, out=False, _in=False)[\"revisions\"][0][str(key)][\"tokens\"]\n",
    "    rev_content = pd.DataFrame(rev_content)\n",
    "    # rev_content[\"added\"] = pd.Series(np.full((len(rev_content),), False))\n",
    "    # rev_content = rev_content.apply(lambda x: was_added(x, key),axis=1)\n",
    "    # del rev_content[\"in\"]\n",
    "    # del rev_content[\"out\"]\n",
    "    rev_content = add_start_end_token(rev_content)\n",
    "    return rev_content\n",
    "\n",
    "def get_str_tokenid_for_rev(rev):\n",
    "    strs = [tok[\"str\"] for tok in ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]]\n",
    "    tokens = [tok[\"token_id\"] for tok in ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]]\n",
    "    rev_len_list = pd.DataFrame({\"str\": strs, \"token_id\": tokens})\n",
    "    rev_len_list = add_start_end_token(rev_len_list, add_false=False)\n",
    "    return rev_len_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = 'Bioglass'\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir = \"../data/change objects/\"\n",
    "epsilon_size=30\n",
    "save=False\n",
    "\n",
    "content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "ww = WikiWho(protocol=\"http\", domain=\"10.6.13.139\")\n",
    "\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "#retrieving rev list\n",
    "# keys = [rev[\"id\"] for rev in ww.dv.api.rev_ids_of_article(\"Bioglass\")[\"revisions\"]]\n",
    "# rev_len_list = []\n",
    "# print(len(keys))\n",
    "# for i, key in enumerate(keys):\n",
    "#     print(i)\n",
    "#     rev_len_list.append(len(ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]) + 2)\n",
    "#     sleep(1)\n",
    "    # use the following instead when running on the machine\n",
    "\n",
    "#     rev_len_list = [len(ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]) + 2 for key in keys]\n",
    "# rev_len_df = pd.DataFrame({\"rev_id\":keys[:-1], \"length\": rev_len_list[:-1]})\n",
    "\n",
    "# rev_len_df.to_hdf(len_file_path, \"rev_len\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r_ids = ww.api.rev_ids_of_article(\"Bioglass\")\n",
    "rev_list = pd.DataFrame(r_ids[\"revisions\"])\n",
    "all_rev = pd.DataFrame(ww.api.all_content(\"Bioglass\")[\"all_tokens\"])\n",
    "del all_rev[\"editor\"]\n",
    "all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "\n",
    "#making revision objects\n",
    "revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "revs.index = rev_list.id\n",
    "\n",
    "\n",
    "# Getting first revision object and adding content ot it\n",
    "from_rev_id = revs.index[0]\n",
    "wiki = Wiki(article_name, revs, all_tokens)\n",
    "\n",
    "wiki.revisions.iloc[0].content = df_rev_content(from_rev_id, first=True)\n",
    "# adding content to all other revision and finding change object between them.\n",
    "\n",
    "print(len(revs))\n",
    "for i, to_rev_id in enumerate(list(revs.index[1:])):\n",
    "    print(i)\n",
    "    to_rev_content = df_rev_content(to_rev_id)\n",
    "    wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "    from_rev_id = to_rev_id\n",
    "    #sleep(1)\n",
    "\n",
    "if save:\n",
    "    with open(change_object_filepath, \"wb\") as file:\n",
    "        pickle.dump(wiki, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- article_name: name of the article\n",
    "- rev_list: ?\n",
    "- revs: is related to rev_list\n",
    "- all_rev: ?\n",
    "- all_tokens: depens on all_rev?\n",
    "- key: seems to be a revision\n",
    "- store[key]: it is the same as to_revision_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving change_object as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def la(rev):\n",
    "    print(rev)\n",
    "    change_objects.append(revision.change_df)\n",
    "\n",
    "\n",
    "change_objects = []\n",
    "wiki.revisions.iloc[:-1].apply(lambda revision: change_objects.append(revision.change_df))\n",
    "# change_index = [ rev.id for rev in  wiki.revisions[1:].tolist()]\n",
    "# change_df = pd.concat(change_objects, sort=False, keys=change_index, axis=)\n",
    "\n",
    "\n",
    "timestamp_s = pd.to_datetime([ rev.timestamp for rev in  wiki.revisions.values.ravel().tolist()])\n",
    "time_gap = pd.to_timedelta(timestamp_s[1:]-timestamp_s[:-1])\n",
    "\n",
    "rev_ids = [ rev.id for rev in  wiki.revisions.tolist()]\n",
    "from_rev_ids = rev_ids[:-1]\n",
    "to_rev_ids= rev_ids[1:]\n",
    "\n",
    "editor_s = [ rev.editor for rev in  wiki.revisions.tolist()]\n",
    "\n",
    "index = list(zip(*[from_rev_ids, to_rev_ids, timestamp_s.tolist()[1:], time_gap, editor_s[1:]]))\n",
    "change_df = pd.concat(change_objects, sort=False, keys=index, names=[\"from revision id\", \"to revision id\", \"timestamp\", \"timegap\", \"editor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# change_object_dir =  \"../data/change objects/\"\n",
    "# change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.pkl\")\n",
    "# a=change_df.to_pickle(change_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# change_object_dir =  \"../data/change objects/\"\n",
    "# change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.pkl\")\n",
    "# a=pd.read_pickle(change_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('../data/change objects')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.h5\")\n",
    "change_df.to_hdf(change_dataframe_path, key=\"data\", mode ='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
