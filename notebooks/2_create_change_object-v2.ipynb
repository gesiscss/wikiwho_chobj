{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import pickle\n",
    "import requests\n",
    "from time import sleep\n",
    "from wikiwho_wrapper import WikiWho\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from scripts.wiki import Wiki,Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki:\n",
    "    '''\n",
    "    MAIN CLASS TO store all revisions for a wiki along with editors and timestamp.\n",
    "    '''\n",
    "    def __init__(self,title, revs, all_tokens=[]):\n",
    "        #self.id = id\n",
    "        self.title = title\n",
    "        self.revisions = revs\n",
    "        self.add_all_token(all_tokens)\n",
    "        \n",
    "\n",
    "           \n",
    "    def add_all_token(self, all_tokens):\n",
    "        \n",
    "        for token in all_tokens:\n",
    "            self.revisions.loc[token[\"o_rev_id\"]].added.add(token[\"token_id\"])\n",
    "            for in_revision in token[\"in\"]:\n",
    "                self.revisions.loc[in_revision].added.add(token[\"token_id\"])\n",
    "            for out_revision in token[\"out\"]:\n",
    "                self.revisions.loc[out_revision].removed.add(token[\"token_id\"])\n",
    "                \n",
    "    def create_change(self, from_rev_id, to_rev_id, to_rev_content, epsilon_size):\n",
    "        try:\n",
    "            from_rev = self.revisions[from_rev_id]\n",
    "            to_rev = self.revisions[to_rev_id]\n",
    "            from_rev.deleted(to_rev)\n",
    "            to_rev.content = to_rev_content\n",
    "            to_rev.inserted_continuous_pos()\n",
    "            to_rev.inserted_neighbours()\n",
    "            from_rev.create_change_object(to_rev)\n",
    "            from_rev.append_neighbour_vec(to_rev, epsilon_size)\n",
    "        except:\n",
    "            print(\"exception occurred in calculating change object\",traceback.format_exc())\n",
    "            print(\"problem in \", to_rev_content.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Revision:\n",
    "    def __init__(self, id, timestamp,editor):\n",
    "        self.id = id\n",
    "        self.timestamp = timestamp\n",
    "        self.editor = editor\n",
    "        self.added = set()\n",
    "        self.removed = set()   \n",
    "        \n",
    "    def deleted(self, to_rev):\n",
    "        self.content[\"removed\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(to_rev.removed), assume_unique= True ))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"removed\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        start_neighbour = start_pos - 1\n",
    "        end_neighbour = end_pos + 1\n",
    "        self.deleted_object = pd.DataFrame(np.c_[ start_pos, end_pos, start_neighbour, end_neighbour ],\n",
    "                                       columns=[ \"del_start_pos\", \"del_end_pos\", \"left_neigh\", \"right_neigh\",])\n",
    "    \n",
    "    def inserted_continuous_pos(self):\n",
    "        self.content[\"added\"] = pd.Series(np.isin( self.content[\"token_id\"].values, list(self.added), assume_unique= True))\n",
    "        end_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == -1) -1 \n",
    "        start_pos = np.argwhere(np.ediff1d(np.pad(self.content[\"added\"].astype(np.int), (1,1), mode=\"constant\", constant_values=0)) == 1)\n",
    "        self.added_pos = np.c_[start_pos, end_pos]\n",
    "\n",
    "    def inserted_neighbours(self):\n",
    "        start_token_pos = self.added_pos[:,0] - 1\n",
    "        end_token_pos = self.added_pos[:,1] + 1\n",
    "        self.start_token_id = self.content[\"token_id\"].values[start_token_pos]\n",
    "        self.end_token_id = self.content[\"token_id\"].values[end_token_pos]\n",
    "    \n",
    "    def create_change_object(self, to_rev):\n",
    "        self.ins_left = np.argwhere(np.isin(self.content.token_id.values, to_rev.start_token_id, assume_unique= True))\n",
    "        self.ins_right = np.argwhere(np.isin(self.content.token_id.values, to_rev.end_token_id, assume_unique= True))\n",
    "        self.inserted_object = pd.DataFrame(np.concatenate([to_rev.added_pos, self.ins_left, self.ins_right], axis=1),\n",
    "                                       columns=[\"ins_start_pos\", \"ins_end_pos\", \"left_neigh\", \"right_neigh\" ])\n",
    "\n",
    "        self.change = pd.merge(self.inserted_object, self.deleted_object,how=\"outer\", on=[\"left_neigh\", \"right_neigh\"])\n",
    "        self.change.fillna(-1, inplace=True)\n",
    "        \n",
    "    def append_neighbour_vec(self, to_rev, epsilon_size):\n",
    "        self.wiki_who_tokens = self.content.token_id.values\n",
    "        del self.content\n",
    "        neighbour_df = self.change.apply(find_tokens, axis=1, args=(self, to_rev, epsilon_size))\n",
    "        neighbour_df.columns= [\"ins_tokens\", \"del_tokens\", \"left_neigh_slice\", \"right_neigh_slice\", \"left_token\", \"right_token\"]\n",
    "        self.change_df = pd.concat([self.change, neighbour_df], sort=False, axis=1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tokens(change, revision, to_rev, epsilon_size):\n",
    "    start_left = (int(change[\"left_neigh\"]) - epsilon_size)\n",
    "    if start_left <0:\n",
    "        start_left = 0\n",
    "    left_neigh = slice( start_left, int(change[\"left_neigh\"]) + 1)\n",
    "    \n",
    "    end_right = (int(change[\"right_neigh\"]) + epsilon_size+1)\n",
    "    if end_right >= revision.wiki_who_tokens.size:\n",
    "        end_right = revision.wiki_who_tokens.size - 1\n",
    "    right_neigh = slice(int(change[\"right_neigh\"]), end_right )\n",
    "    if(change[\"ins_start_pos\"]==-1):\n",
    "        ins_tokens = []\n",
    "    else:\n",
    "        ins_slice = slice(int(change[\"ins_start_pos\"]), int(change[\"ins_end_pos\"]+1) )\n",
    "        ins_tokens = to_rev.content.token_id.values[ins_slice]\n",
    "    if(change[\"del_start_pos\"] == -1):\n",
    "        del_tokens = []\n",
    "    else:\n",
    "        del_slice = slice(int(change[\"del_start_pos\"]), int(change[\"del_end_pos\"]+1) )\n",
    "        del_tokens = revision.wiki_who_tokens[del_slice]\n",
    "    left_token = revision.wiki_who_tokens[left_neigh]\n",
    "    right_token = revision.wiki_who_tokens[right_neigh]\n",
    "    return pd.Series([tuple(ins_tokens), tuple(del_tokens), left_neigh, right_neigh, tuple(left_token), tuple(right_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://api.wikiwho.net/en/api/v1.0.0-beta/\"\n",
    "article_name = \"Bioglass\"\n",
    "filename = article_name + \".h5\"\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir =  \"../data/change objects/\"\n",
    "filepath = os.path.join(content_dir, filename)\n",
    "\n",
    "epsilon_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``../data/content/Bioglass.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-81132e15ed36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlen_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDFStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#retrieving all rev list and change object from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrev_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rev_list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/wikiconflict/lib/python3.7/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, complevel, complib, fletcher32, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fletcher32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfletcher32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/wikiconflict/lib/python3.7/site-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'can not be written'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/wikiconflict/lib/python3.7/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/wikiconflict/lib/python3.7/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/wikiconflict/lib/python3.7/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# The file should be readable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` does not exist\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ``../data/content/Bioglass.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "with pd.HDFStore(filepath, 'r') as store:\n",
    "    #retrieving all rev list and change object from file\n",
    "    rev_list = store.get(\"rev_list\")[\"id\"].values.tolist()\n",
    "    keys = [\"r\" +  str(rev) for rev in rev_list]\n",
    "    rev_len_list = [store.get(key).shape[0] for key in keys]\n",
    "rev_len_df = pd.DataFrame({\"rev_id\":rev_list[:-1], \"length\": rev_len_list[:-1]})\n",
    "\n",
    "rev_len_df.to_hdf(len_file_path, \"rev_len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# with pd.HDFStore(filepath, 'r') as store:\n",
    "#     #retrieving all rev list and change object from file\n",
    "#     rev_list = store.get(\"rev_list\")\n",
    "#     all_rev = store.get(\"all_tokens\")\n",
    "#     all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "#     #making revision objects\n",
    "#     revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "#     revs.index = rev_list.id\n",
    "#     from_rev_id = revs.index[0]\n",
    "    \n",
    "#     wiki = Wiki(2345, content, revs, all_tokens)\n",
    "#     wiki.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "#     for to_rev_id in list(revs.index[1:]):\n",
    "#         key=\"r\"+str(to_rev_id)\n",
    "#         to_rev_content = store[key]\n",
    "#         wiki\n",
    "#         wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "#         from_rev_id = to_rev_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_filepath = os.path.join(change_object_dir, content+\".pkl\")\n",
    "# with open(save_filepath, \"wb\") as file:\n",
    "#     pickle.dump(wiki, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving change object for all the articles in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_change_object(article_name, content_dir = \"../data/content/\", \n",
    "                            change_object_dir =  \"../data/change objects/\", epsilon_size=30, save=False):\n",
    "    \n",
    "    content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "    change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "    \n",
    "    with pd.HDFStore(content_filepath, 'r') as store:\n",
    "        #retrieving all rev list and change object from file\n",
    "        rev_list = store.get(\"rev_list\")\n",
    "        all_rev = store.get(\"all_tokens\")\n",
    "        all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "        \n",
    "        #making revision objects\n",
    "        revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "        revs.index = rev_list.id\n",
    "        \n",
    "        # Getting first revision object and adding content ot it\n",
    "        from_rev_id = revs.index[0]\n",
    "        wiki = Wiki(article_name, revs, all_tokens)\n",
    "\n",
    "        wiki.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "        # adding content to all other revision and finding change object between them.\n",
    "        \n",
    "        for to_rev_id in list(revs.index[1:]):\n",
    "            key=\"r\"+str(to_rev_id)\n",
    "            to_rev_content = store[key]\n",
    "            wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "            from_rev_id = to_rev_id\n",
    "         \n",
    "    if save:\n",
    "        with open(change_object_filepath, \"wb\") as file:\n",
    "            pickle.dump(wiki, file)\n",
    "        \n",
    "    return wiki\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = 'Bioglass'\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir = \"../data/change objects/\"\n",
    "epsilon_size=30\n",
    "save=False\n",
    "\n",
    "content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "\n",
    "with pd.HDFStore(content_filepath, 'r') as store:\n",
    "    #retrieving all rev list and change object from file\n",
    "    rev_list = store.get(\"rev_list\")\n",
    "    all_rev = store.get(\"all_tokens\")\n",
    "    all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "\n",
    "    #making revision objects\n",
    "    revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "    revs.index = rev_list.id\n",
    "\n",
    "    # Getting first revision object and adding content ot it\n",
    "    from_rev_id = revs.index[0]\n",
    "    wiki2 = Wiki(article_name, revs, all_tokens)\n",
    "\n",
    "    wiki2.revisions.iloc[0].content = store[\"r\"+str(from_rev_id)] \n",
    "    # adding content to all other revision and finding change object between them.\n",
    "\n",
    "    for to_rev_id in list(revs.index[1:]):\n",
    "        key=\"r\"+str(to_rev_id)\n",
    "        to_rev_content = store[key]\n",
    "        wiki2.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "        from_rev_id = to_rev_id\n",
    "\n",
    "if save:\n",
    "    with open(change_object_filepath, \"wb\") as file:\n",
    "        pickle.dump(wiki2, file)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract this data using the wikiwho wrapper instead of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def was_added(x, revision):\n",
    "    i = x[\"in\"]\n",
    "    o = x[\"out\"]\n",
    "    i_cleaned = []\n",
    "    o_cleaned = []\n",
    "    for rev in i:\n",
    "        if rev <= revision:\n",
    "            i_cleaned.append(rev)\n",
    "    for rev in o:\n",
    "        if rev <= revision:\n",
    "            o_cleaned.append(rev)\n",
    "    if (len(i_cleaned) == len(o_cleaned)):\n",
    "        x.added = True\n",
    "    return x\n",
    "\n",
    "def add_start_end_token(df, add_false=False):\n",
    "    if add_false:\n",
    "        df.loc[len(df)] = [\"{$nd}\", -2, False]\n",
    "        df.loc[-1] = [\"{st@rt}\", -1, False]\n",
    "    else:\n",
    "        df.loc[len(df)] = [\"{$nd}\", -2]\n",
    "        df.loc[-1] = [\"{st@rt}\", -1]      \n",
    "    df.index = df.index+1\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def df_rev_content(key, first=False):\n",
    "    rev_content = ww.api.specific_rev_content_by_rev_id(key, o_rev_id=False, editor=False, out=False, _in=False)[\"revisions\"][0][str(key)][\"tokens\"]\n",
    "    rev_content = pd.DataFrame(rev_content)\n",
    "    # rev_content[\"added\"] = pd.Series(np.full((len(rev_content),), False))\n",
    "    # rev_content = rev_content.apply(lambda x: was_added(x, key),axis=1)\n",
    "    # del rev_content[\"in\"]\n",
    "    # del rev_content[\"out\"]\n",
    "    rev_content = add_start_end_token(rev_content)\n",
    "    return rev_content\n",
    "\n",
    "def get_str_tokenid_for_rev(rev):\n",
    "    strs = [tok[\"str\"] for tok in ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]]\n",
    "    tokens = [tok[\"token_id\"] for tok in ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]]\n",
    "    rev_len_list = pd.DataFrame({\"str\": strs, \"token_id\": tokens})\n",
    "    rev_len_list = add_start_end_token(rev_len_list, add_false=False)\n",
    "    return rev_len_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_name = 'Bioglass'\n",
    "content_dir = \"../data/content/\"\n",
    "change_object_dir = \"../data/change objects/\"\n",
    "epsilon_size=30\n",
    "save=False\n",
    "\n",
    "content_filepath = os.path.join(content_dir, article_name+\".h5\")\n",
    "change_object_filepath = os.path.join(change_object_dir, article_name+\".pkl\")\n",
    "ww = WikiWho(protocol=\"http\", domain=\"10.6.13.139\")\n",
    "\n",
    "len_file = article_name + \"_rev_len.h5\"\n",
    "len_file_path = os.path.join(content_dir, len_file)\n",
    "\n",
    "#retrieving rev list\n",
    "# keys = [rev[\"id\"] for rev in ww.dv.api.rev_ids_of_article(\"Bioglass\")[\"revisions\"]]\n",
    "# rev_len_list = []\n",
    "# print(len(keys))\n",
    "# for i, key in enumerate(keys):\n",
    "#     print(i)\n",
    "#     rev_len_list.append(len(ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]) + 2)\n",
    "#     sleep(1)\n",
    "    # use the following instead when running on the machine\n",
    "\n",
    "#     rev_len_list = [len(ww.dv.api.specific_rev_content_by_rev_id(key)[\"revisions\"][0][str(key)][\"tokens\"]) + 2 for key in keys]\n",
    "# rev_len_df = pd.DataFrame({\"rev_id\":keys[:-1], \"length\": rev_len_list[:-1]})\n",
    "\n",
    "# rev_len_df.to_hdf(len_file_path, \"rev_len\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r_ids = ww.api.rev_ids_of_article(\"Bioglass\")\n",
    "rev_list = pd.DataFrame(r_ids[\"revisions\"])\n",
    "all_rev = pd.DataFrame(ww.api.all_content(\"Bioglass\")[\"all_tokens\"])\n",
    "del all_rev[\"editor\"]\n",
    "all_tokens = all_rev.to_dict(orient=\"records\")\n",
    "\n",
    "#making revision objects\n",
    "revs = rev_list.apply(lambda rev: Revision(rev[\"id\"],rev[\"timestamp\"], rev[\"editor\"]),axis=1)\n",
    "revs.index = rev_list.id\n",
    "\n",
    "\n",
    "# Getting first revision object and adding content ot it\n",
    "from_rev_id = revs.index[0]\n",
    "wiki = Wiki(article_name, revs, all_tokens)\n",
    "\n",
    "wiki.revisions.iloc[0].content = df_rev_content(from_rev_id, first=True)\n",
    "# adding content to all other revision and finding change object between them.\n",
    "\n",
    "print(len(revs))\n",
    "for i, to_rev_id in enumerate(list(revs.index[1:])):\n",
    "    print(i)\n",
    "    to_rev_content = df_rev_content(to_rev_id)\n",
    "    wiki.create_change(from_rev_id, to_rev_id, to_rev_content, epsilon_size)\n",
    "    from_rev_id = to_rev_id\n",
    "    #sleep(1)\n",
    "\n",
    "if save:\n",
    "    with open(change_object_filepath, \"wb\") as file:\n",
    "        pickle.dump(wiki, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- article_name: name of the article\n",
    "- rev_list: ?\n",
    "- revs: is related to rev_list\n",
    "- all_rev: ?\n",
    "- all_tokens: depens on all_rev?\n",
    "- key: seems to be a revision\n",
    "- store[key]: it is the same as to_revision_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving change_object as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def la(rev):\n",
    "    print(rev)\n",
    "    change_objects.append(revision.change_df)\n",
    "\n",
    "\n",
    "change_objects = []\n",
    "wiki.revisions.iloc[:-1].apply(lambda revision: change_objects.append(revision.change_df))\n",
    "# change_index = [ rev.id for rev in  wiki.revisions[1:].tolist()]\n",
    "# change_df = pd.concat(change_objects, sort=False, keys=change_index, axis=)\n",
    "\n",
    "\n",
    "timestamp_s = pd.to_datetime([ rev.timestamp for rev in  wiki.revisions.values.ravel().tolist()])\n",
    "time_gap = pd.to_timedelta(timestamp_s[1:]-timestamp_s[:-1])\n",
    "\n",
    "rev_ids = [ rev.id for rev in  wiki.revisions.tolist()]\n",
    "from_rev_ids = rev_ids[:-1]\n",
    "to_rev_ids= rev_ids[1:]\n",
    "\n",
    "editor_s = [ rev.editor for rev in  wiki.revisions.tolist()]\n",
    "\n",
    "index = list(zip(*[from_rev_ids, to_rev_ids, timestamp_s.tolist()[1:], time_gap, editor_s[1:]]))\n",
    "change_df = pd.concat(change_objects, sort=False, keys=index, names=[\"from revision id\", \"to revision id\", \"timestamp\", \"timegap\", \"editor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# change_object_dir =  \"../data/change objects/\"\n",
    "# change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.pkl\")\n",
    "# a=change_df.to_pickle(change_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# change_object_dir =  \"../data/change objects/\"\n",
    "# change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.pkl\")\n",
    "# a=pd.read_pickle(change_dataframe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('../data/change objects')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "change_dataframe_path = os.path.join(change_object_dir, article_name+\"_change.h5\")\n",
    "change_df.to_hdf(change_dataframe_path, key=\"data\", mode ='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
